{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0613ecd-0a9b-49e7-82d4-e865575f2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO, SAM\n",
    "from PIL import Image, ImageFilter\n",
    "import pandas as pd\n",
    "\n",
    "# Load models\n",
    "model = YOLO('./runs/detect/train27/weights/best.pt')\n",
    "sam_model = SAM('sam2.1_b.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf5997b-568a-48c2-8830-6838e19360cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 6 Faces, 14.3ms\n",
      "Speed: 2.2ms preprocess, 14.3ms inference, 70.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 1024x1024 1 0, 183.8ms\n",
      "Speed: 2.9ms preprocess, 183.8ms inference, 8.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 145.8ms\n",
      "Speed: 2.7ms preprocess, 145.8ms inference, 0.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 145.0ms\n",
      "Speed: 2.7ms preprocess, 145.0ms inference, 0.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 140.8ms\n",
      "Speed: 2.8ms preprocess, 140.8ms inference, 0.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 146.8ms\n",
      "Speed: 2.7ms preprocess, 146.8ms inference, 0.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 1 0, 146.2ms\n",
      "Speed: 2.6ms preprocess, 146.2ms inference, 0.4ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Load image using PIL\n",
    "image_pil = Image.open(\"./test/f1.jpg\").convert(\"RGB\")\n",
    "image_np = np.array(image_pil)\n",
    "\n",
    "# Create blurred image using PIL\n",
    "blurred_image_pil = image_pil.filter(ImageFilter.GaussianBlur(radius=25)) \n",
    "blurred_image_np = np.array(blurred_image_pil)\n",
    "\n",
    "\n",
    "# Object detection and processing\n",
    "results = model(image_np)\n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    for box in boxes:\n",
    "        cls = int(box.cls)\n",
    "        conf = box.conf\n",
    "        if model.names[cls] == \"Face\" and conf > 0.5:\n",
    "            xyxy = box.xyxy[0].tolist()\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "\n",
    "            # SAM segmentation\n",
    "            input_box = np.array([x1, y1, x2, y2]).reshape(1, 4)\n",
    "            sam_result = sam_model.predict(image_pil, bboxes=input_box)\n",
    "            mask = sam_result[0].masks.data[0].cpu().numpy().astype(np.uint8)\n",
    "\n",
    "            # Apply blur\n",
    "            image_np[mask == 1] = blurred_image_np[mask == 1]\n",
    "\n",
    "# Display image using PIL\n",
    "image_pil_blurred = Image.fromarray(image_np)\n",
    "image_pil_blurred.show()  # image_pil_blurred.save(\"output.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6190b90-3ca1-4d14-8403-74a4e4ba677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(process:49596): Gtk-WARNING **: 13:50:41.374: Locale not supported by C library.\n",
      "\tUsing the fallback 'C' locale.\n",
      "** Message: 13:50:41.405: main.vala:507: Starting session with system profile\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('/mnt/WD/ds-phase-2/09-cv/Cancer_3/last_axial_coronal/weights/best.pt')\n",
    "sam_model = SAM('sam2.1_b.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79133da9-5084-43b8-a81b-5954b090cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x512 1 negative, 3.4ms\n",
      "Speed: 1.7ms preprocess, 3.4ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 512)\n",
      "\n",
      "0: 1024x1024 1 0, 151.9ms\n",
      "Speed: 3.1ms preprocess, 151.9ms inference, 0.3ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(process:50690): Gtk-WARNING **: 13:53:00.479: Locale not supported by C library.\n",
      "\tUsing the fallback 'C' locale.\n",
      "** Message: 13:53:00.507: main.vala:507: Starting session with system profile\n"
     ]
    }
   ],
   "source": [
    "image_pil = Image.open(\"/mnt/WD/09_data/c2/images/test/00072_81.jpg\").convert(\"RGB\")\n",
    "image_np = np.array(image_pil)\n",
    "# Create a transparent overlay (PIL Image)\n",
    "overlay = Image.new(\"RGBA\", image_pil.size, (0, 0, 0, 0))\n",
    "\n",
    "# Object detection and processing\n",
    "results = model(image_np)\n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    for box in boxes:\n",
    "        cls = int(box.cls)\n",
    "        conf = box.conf\n",
    "        if model.names[cls] == \"negative\" and conf > 0.4:\n",
    "            xyxy = box.xyxy[0].tolist()\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "\n",
    "            # SAM segmentation\n",
    "            input_box = np.array([x1, y1, x2, y2]).reshape(1, 4)\n",
    "            sam_result = sam_model.predict(image_pil, bboxes=input_box)\n",
    "            mask = sam_result[0].masks.data[0].cpu().numpy().astype(np.uint8)\n",
    "\n",
    "            # Apply transparent overlay using PIL\n",
    "            mask_pil = Image.fromarray(mask * 255).convert(\"L\")\n",
    "            overlay.paste((0, 255, 0, 128), box=None, mask=mask_pil)\n",
    "            \n",
    "        if model.names[cls] == \"positive\" and conf > 0.4:\n",
    "            xyxy = box.xyxy[0].tolist()\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "\n",
    "            # SAM segmentation\n",
    "            input_box = np.array([x1, y1, x2, y2]).reshape(1, 4)\n",
    "            sam_result = sam_model.predict(image_pil, bboxes=input_box)\n",
    "            mask = sam_result[0].masks.data[0].cpu().numpy().astype(np.uint8)\n",
    "\n",
    "            # Apply transparent overlay using PIL\n",
    "            mask_pil = Image.fromarray(mask * 255).convert(\"L\")  # Convert mask to grayscale PIL image\n",
    "            overlay.paste((255, 0, 0, 128), box=None, mask=mask_pil)  # Paste semi-transparent black over the mask\n",
    "\n",
    "\n",
    "# Composite the original image and the overlay\n",
    "image_with_overlay = Image.composite(overlay, image_pil, overlay)\n",
    "\n",
    "# Display or save\n",
    "image_with_overlay.show()\n",
    "#image_with_overlay.save(\"output_with_overlay.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747c3ce-04ad-470e-911c-58b2ab5cccfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
